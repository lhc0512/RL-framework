{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to : cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "################################## set device ##################################\n",
    "\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n",
    "else:\n",
    "    print(\"Device set to : cpu\")\n",
    "\n",
    "################################## PPO Policy ##################################\n",
    "\n",
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "\n",
    "\n",
    "    def clear(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.is_terminals[:]\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "                        nn.Linear(state_dim, 64),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(64, 64),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(64, action_dim),\n",
    "                        nn.Softmax(dim=-1)\n",
    "                    )\n",
    "\n",
    "        # critic\n",
    "        self.critic = nn.Sequential(\n",
    "                        nn.Linear(state_dim, 64),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(64, 64),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(64, 1)\n",
    "                    )\n",
    "\n",
    "    def act(self, state):\n",
    "        action_probs = self.actor(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "        return action.detach(), action_logprob.detach()\n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "        action_probs = self.actor(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "\n",
    "        dist_entropy = dist.entropy()\n",
    "        state_values = self.critic(state)\n",
    "\n",
    "        return action_logprobs, state_values, dist_entropy\n",
    "\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip):\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "\n",
    "        self.buffer = RolloutBuffer()\n",
    "\n",
    "        self.policy = ActorCritic(state_dim, action_dim).to(device)\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
    "                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
    "                    ])\n",
    "\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        self.MseLoss = nn.MSELoss()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        action, action_logprob = self.policy_old.act(state)\n",
    "\n",
    "        self.buffer.states.append(state)\n",
    "        self.buffer.actions.append(action)\n",
    "        self.buffer.logprobs.append(action_logprob)\n",
    "\n",
    "        return action.item()\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "        # Monte Carlo estimate of returns\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "\n",
    "        # Normalizing the rewards\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
    "\n",
    "        # convert list to tensor\n",
    "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)\n",
    "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)\n",
    "        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device)\n",
    "\n",
    "\n",
    "        # Optimize policy for K epochs\n",
    "        for _ in range(self.K_epochs):\n",
    "\n",
    "            # Evaluating old actions and values\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "\n",
    "            # match state_values tensor dimensions with rewards tensor\n",
    "            state_values = torch.squeeze(state_values)\n",
    "\n",
    "            # Finding the ratio (pi_theta / pi_theta__old)\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "            # Finding Surrogate Loss\n",
    "            advantages = rewards - state_values.detach()\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "\n",
    "            # final loss of clipped objective PPO\n",
    "            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n",
    "\n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Copy new weights into old policy\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        # clear buffer\n",
    "        self.buffer.clear()\n",
    "\n",
    "\n",
    "    def save(self, checkpoint_path):\n",
    "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
    "\n",
    "\n",
    "    def load(self, checkpoint_path):\n",
    "        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
    "        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save checkpoint path : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
      "Episode : 73 \t\t Timestep : 1600 \t\t Average Reward : 21.66\n",
      "Episode : 135 \t\t Timestep : 3200 \t\t Average Reward : 25.53\n",
      "Episode : 182 \t\t Timestep : 4800 \t\t Average Reward : 34.57\n",
      "Episode : 211 \t\t Timestep : 6400 \t\t Average Reward : 54.55\n",
      "Episode : 228 \t\t Timestep : 8000 \t\t Average Reward : 91.47\n",
      "Episode : 240 \t\t Timestep : 9600 \t\t Average Reward : 131.33\n",
      "Episode : 252 \t\t Timestep : 11200 \t\t Average Reward : 141.08\n",
      "Episode : 258 \t\t Timestep : 12800 \t\t Average Reward : 240.17\n",
      "Episode : 267 \t\t Timestep : 14400 \t\t Average Reward : 188.89\n",
      "Episode : 274 \t\t Timestep : 16000 \t\t Average Reward : 212.71\n",
      "Episode : 281 \t\t Timestep : 17600 \t\t Average Reward : 218.86\n",
      "Episode : 286 \t\t Timestep : 19200 \t\t Average Reward : 323.8\n",
      "Episode : 293 \t\t Timestep : 20800 \t\t Average Reward : 247.86\n",
      "Episode : 299 \t\t Timestep : 22400 \t\t Average Reward : 264.33\n",
      "Episode : 303 \t\t Timestep : 24000 \t\t Average Reward : 374.5\n",
      "Episode : 309 \t\t Timestep : 25600 \t\t Average Reward : 274.83\n",
      "Episode : 314 \t\t Timestep : 27200 \t\t Average Reward : 342.0\n",
      "Episode : 319 \t\t Timestep : 28800 \t\t Average Reward : 315.8\n",
      "Episode : 325 \t\t Timestep : 30400 \t\t Average Reward : 255.33\n",
      "Episode : 329 \t\t Timestep : 32000 \t\t Average Reward : 357.25\n",
      "Episode : 333 \t\t Timestep : 33600 \t\t Average Reward : 380.75\n",
      "Episode : 337 \t\t Timestep : 35200 \t\t Average Reward : 396.25\n",
      "Episode : 342 \t\t Timestep : 36800 \t\t Average Reward : 370.8\n",
      "Episode : 347 \t\t Timestep : 38400 \t\t Average Reward : 347.6\n",
      "Episode : 352 \t\t Timestep : 40000 \t\t Average Reward : 320.2\n",
      "Episode : 356 \t\t Timestep : 41600 \t\t Average Reward : 400.0\n",
      "Episode : 360 \t\t Timestep : 43200 \t\t Average Reward : 400.0\n",
      "Episode : 364 \t\t Timestep : 44800 \t\t Average Reward : 400.0\n",
      "Episode : 368 \t\t Timestep : 46400 \t\t Average Reward : 400.0\n",
      "Episode : 372 \t\t Timestep : 48000 \t\t Average Reward : 349.25\n",
      "Episode : 376 \t\t Timestep : 49600 \t\t Average Reward : 400.0\n",
      "Episode : 380 \t\t Timestep : 51200 \t\t Average Reward : 400.0\n",
      "Episode : 384 \t\t Timestep : 52800 \t\t Average Reward : 400.0\n",
      "Episode : 388 \t\t Timestep : 54400 \t\t Average Reward : 397.0\n",
      "Episode : 392 \t\t Timestep : 56000 \t\t Average Reward : 392.5\n",
      "Episode : 396 \t\t Timestep : 57600 \t\t Average Reward : 397.75\n",
      "Episode : 401 \t\t Timestep : 59200 \t\t Average Reward : 363.8\n",
      "Episode : 405 \t\t Timestep : 60800 \t\t Average Reward : 386.5\n",
      "Episode : 409 \t\t Timestep : 62400 \t\t Average Reward : 400.0\n",
      "Episode : 413 \t\t Timestep : 64000 \t\t Average Reward : 373.25\n",
      "Episode : 417 \t\t Timestep : 65600 \t\t Average Reward : 383.5\n",
      "Episode : 421 \t\t Timestep : 67200 \t\t Average Reward : 400.0\n",
      "Episode : 427 \t\t Timestep : 68800 \t\t Average Reward : 302.33\n",
      "Episode : 431 \t\t Timestep : 70400 \t\t Average Reward : 379.5\n",
      "Episode : 436 \t\t Timestep : 72000 \t\t Average Reward : 306.8\n",
      "Episode : 442 \t\t Timestep : 73600 \t\t Average Reward : 270.5\n",
      "Episode : 447 \t\t Timestep : 75200 \t\t Average Reward : 333.8\n",
      "Episode : 451 \t\t Timestep : 76800 \t\t Average Reward : 397.0\n",
      "Episode : 455 \t\t Timestep : 78400 \t\t Average Reward : 336.25\n",
      "Episode : 459 \t\t Timestep : 80000 \t\t Average Reward : 400.0\n",
      "Episode : 463 \t\t Timestep : 81600 \t\t Average Reward : 400.0\n",
      "Episode : 468 \t\t Timestep : 83200 \t\t Average Reward : 388.6\n",
      "Episode : 472 \t\t Timestep : 84800 \t\t Average Reward : 391.25\n",
      "Episode : 476 \t\t Timestep : 86400 \t\t Average Reward : 371.75\n",
      "Episode : 480 \t\t Timestep : 88000 \t\t Average Reward : 400.0\n",
      "Episode : 484 \t\t Timestep : 89600 \t\t Average Reward : 388.0\n",
      "Episode : 488 \t\t Timestep : 91200 \t\t Average Reward : 392.75\n",
      "Episode : 492 \t\t Timestep : 92800 \t\t Average Reward : 400.0\n",
      "Episode : 496 \t\t Timestep : 94400 \t\t Average Reward : 390.0\n",
      "Episode : 501 \t\t Timestep : 96000 \t\t Average Reward : 318.2\n",
      "Episode : 505 \t\t Timestep : 97600 \t\t Average Reward : 400.0\n",
      "Episode : 509 \t\t Timestep : 99200 \t\t Average Reward : 400.0\n"
     ]
    }
   ],
   "source": [
    "####### initialize environment hyperparameters ######\n",
    "env_name = \"CartPole-v1\"\n",
    "max_ep_len = 400                    # max timesteps in one episode\n",
    "max_training_timesteps = int(1e5)   # break training loop if timeteps > max_training_timesteps\n",
    "\n",
    "print_freq = max_ep_len * 4     # print avg reward in the interval (in num timesteps)\n",
    "save_model_freq = int(2e4)      # save model frequency (in num timesteps)\n",
    "\n",
    "################ PPO hyperparameters ################\n",
    "update_timestep = max_ep_len * 4      # update policy every n timesteps\n",
    "K_epochs = 40               # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "gamma = 0.99                # discount factor\n",
    "\n",
    "lr_actor = 0.0003       # learning rate for actor network\n",
    "lr_critic = 0.001       # learning rate for critic network\n",
    "\n",
    "random_seed = 0         # set random seed if required (0 = no random seed)\n",
    "\n",
    "\n",
    "env = gym.make(env_name)\n",
    "# state space dimension\n",
    "state_dim = env.observation_space.shape[0]\n",
    "# action space dimension\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "################### checkpointing ###################\n",
    "\n",
    "run_num_pretrained = 0      #### change this to prevent overwriting weights in same env_name folder\n",
    "\n",
    "directory = \"PPO_preTrained\"\n",
    "if not os.path.exists(directory):\n",
    "      os.makedirs(directory)\n",
    "\n",
    "directory = directory + '/' + env_name + '/'\n",
    "if not os.path.exists(directory):\n",
    "      os.makedirs(directory)\n",
    "\n",
    "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
    "print(\"save checkpoint path : \" + checkpoint_path)\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "env.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "################# training procedure ################\n",
    "# initialize a PPO agent\n",
    "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip)\n",
    "# printing and logging variables\n",
    "print_running_reward = 0\n",
    "print_running_episodes = 0\n",
    "\n",
    "time_step = 0\n",
    "i_episode = 0\n",
    "\n",
    "# training loop\n",
    "while time_step <= max_training_timesteps:\n",
    "    state = env.reset()\n",
    "    current_ep_reward = 0\n",
    "\n",
    "    for t in range(1, max_ep_len+1):\n",
    "        # select action with policy\n",
    "        action = ppo_agent.select_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # saving reward and is_terminals\n",
    "        ppo_agent.buffer.rewards.append(reward)\n",
    "        ppo_agent.buffer.is_terminals.append(done)\n",
    "\n",
    "        time_step +=1\n",
    "        current_ep_reward += reward\n",
    "\n",
    "        # update PPO agent\n",
    "        if time_step % update_timestep == 0:\n",
    "            ppo_agent.update()\n",
    "\n",
    "        # printing average reward\n",
    "        if time_step % print_freq == 0:\n",
    "            # print average reward till last episode\n",
    "            print_avg_reward = print_running_reward / print_running_episodes\n",
    "            print_avg_reward = round(print_avg_reward, 2)\n",
    "            print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward))\n",
    "            print_running_reward = 0\n",
    "            print_running_episodes = 0\n",
    "\n",
    "        # save model weights\n",
    "        if time_step % save_model_freq == 0:\n",
    "            ppo_agent.save(checkpoint_path)\n",
    "\n",
    "        # break; if the episode is over\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print_running_reward += current_ep_reward\n",
    "    print_running_episodes += 1\n",
    "    i_episode += 1\n",
    "\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}